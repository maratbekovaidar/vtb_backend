Сначала идет подключение начальных нужных библиотек. После этого, подключаемся к API который был создан бэком. Тут нюанс, туннель может временами оборваться, и придется мануально писать токены и API. Дальше выгрузка данных из json, корректировка и создание датасета. Приступаем к Латентному размещению Дирихле, который показывает количество слов для каждого топика, которые были использованы по максимуму. Тут тоже один нюанс, в коде надо будет написать свой путь для того чтобы сохранить конечный html файл и чтобы jupyter открыл его. Следующая часть - wordcounter. Показывает картинку наиболее использованных слов в целом. От туда взяли массив и исопльзовали его для того чтобы в конечном датасете выявили наиболее трендовые новости с дайджестом. Description является дайджестом, сам датасед конечным наборов трендов.